{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/rdorff/DeepLearning/blob/master/Lab6.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "0VcDIjgnmZbG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n",
        "! tar -xzf text_files.tar.gz\n",
        "! pip install unidecode\n",
        "! pip install torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mqyDCqX5oSWp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c594186-14b8-464b-8d27-6aaa4883fba4"
      },
      "cell_type": "code",
      "source": [
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        " \n",
        "import pdb\n",
        " \n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        " \n",
        "file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_len = 2579888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fprG3Xubo0to",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6552f2e2-c841-4a1b-be25-4ad670249580"
      },
      "cell_type": "code",
      "source": [
        "chunk_len = 200\n",
        " \n",
        "def random_chunk():\n",
        "    start_index = random.randint(0, file_len - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file[start_index:end_index]\n",
        " \n",
        "print(random_chunk())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e); and the guests were selected from all the families to which \n",
            "Bilbo and Frodo were related, with the addition of a few special unrelated \n",
            "friends (such as Gandalf). Many young hobbits were included,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6zti7iCUo5IR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "45081476-f0a6-4d79-db4d-ad089c9e5c5c"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = all_characters.index(string[c])\n",
        "    return Variable(tensor)\n",
        " \n",
        "print(char_tensor('at its foot three Elves were'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([10, 29, 94, 18, 29, 28, 94, 15, 24, 24, 29, 94, 29, 17, 27, 14, 14, 94,\n",
            "        40, 21, 31, 14, 28, 94, 32, 14, 27, 14])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lpn36gmEpCwn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def random_training_set():    \n",
        "    chunk = random_chunk()\n",
        "    inp = char_tensor(chunk[:-1])\n",
        "    target = char_tensor(chunk[1:])\n",
        "    return inp, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U1Mu7rrkpofe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        " \n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        " \n",
        " \n",
        "        # encode using embedding layer\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        # set up GRU passing in number of layers parameter (nn.GRU)\n",
        "        self.gru = GRU(input_size, hidden_size, n_layers)\n",
        "        \n",
        "        # decode output   \n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        " \n",
        "    def forward(self, input_char, hidden):\n",
        "        # by reviewing the documentation, construct a forward function that properly uses the output\n",
        "        # of the GRU\n",
        "        \n",
        "        output = self.embedding(input_char).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        \n",
        "        return output, hidden\n",
        "        \n",
        "\n",
        "    def init_hidden(self):\n",
        "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "URdH7WBsmIqa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, n_layers=1):\n",
        "    super(GRU, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.n_layers = n_layers\n",
        "    self.x2h = nn.Linear(input_size, 3*hidden_size)\n",
        "    self.h2h = nn.Linear(hidden_size, 3*hidden_size)\n",
        "\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    x = x.view(x.size(1),-1)\n",
        "   \n",
        "    gate_x = self.x2h(x) .squeeze()\n",
        "    gate_h = self.h2h(hidden).squeeze()\n",
        "    \n",
        "    i_r, i_i, i_n = gate_x.chunk(3, 0)\n",
        "    h_r, h_i, h_n = gate_h.chunk(3, 0)\n",
        "        \n",
        "        \n",
        "    reset_gate = torch.sigmoid(i_r + h_r)\n",
        "    update_gate = torch.sigmoid(i_i + h_i)\n",
        "    new_gate = torch.tanh(i_n + (reset_gate * h_n))\n",
        "\n",
        "    h_t = new_gate - new_gate * update_gate + update_gate * hidden \n",
        "    \n",
        "    return h_t, h_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NhqOW57htH0M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(inp, target, decoder, decoder_optimzier, criterion):\n",
        "    ## initialize hidden layers, set up gradient and loss \n",
        "      # your code here\n",
        "    ## /\n",
        "    hidden = decoder.init_hidden()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    \n",
        "    for c in range(chunk_len):\n",
        "        output, hidden = decoder(inp[c], hidden) # run the forward pass of your rnn with proper input\n",
        "        loss += criterion(output, target[c].unsqueeze(0))   \n",
        "   \n",
        "    \n",
        "    ## calculate backwards loss and step the optimizer (globally)\n",
        "      # your code here\n",
        "    ## /\n",
        " \n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "  \n",
        "  \n",
        "    return loss.item() / chunk_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ruTBYgcKiYMd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(decoder, prime_str='A', predict_len=100, temperature=0.8):\n",
        "    ## initialize hidden variable, initialize other useful variables \n",
        "      # your code here\n",
        "    ## /\n",
        "    hidden = decoder.init_hidden()\n",
        "    predicted = prime_str\n",
        "    \n",
        "      \n",
        "    prime_input = char_tensor(prime_str)\n",
        " \n",
        "    # Use priming string to \"build up\" hidden state\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = decoder(prime_input[p], hidden)\n",
        "        \n",
        "    inp = prime_input[-1]\n",
        " \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = decoder(inp, hidden) #run your RNN/decoder forward on the input\n",
        "      \n",
        " \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        " \n",
        "        ## get character from your list of all characters, add it to your output str sequence, set input\n",
        "        ## for the next pass through the model\n",
        "         # your code here\n",
        "        ## /\n",
        "        \n",
        "        predicted = predicted + all_characters[top_i]\n",
        " \n",
        "    return predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1zU2h_zVpFpg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1037
        },
        "outputId": "cb11cae0-5dda-49ff-8da7-b8dbb1d32fca"
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "n_epochs = 2000\n",
        "print_every = 100\n",
        "plot_every = 10\n",
        "hidden_size = 100\n",
        "n_layers = 1\n",
        "lr = 0.005\n",
        " \n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        " \n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    loss_ = train(*random_training_set(), decoder, decoder_optimizer, criterion)       \n",
        "    loss_avg += loss_\n",
        "    \n",
        " \n",
        "    if epoch % print_every == 0:\n",
        "        print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "        print(evaluate(decoder, 'Wh', 100), '\\n')\n",
        " \n",
        "    if epoch % plot_every == 0:\n",
        "        all_losses.append(loss_avg / plot_every)\n",
        "        loss_avg = 0"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[18.380439519882202 (100 5%) 2.4508]\n",
            "Wheaeeee,eepeeeaee ueeeeeei,ieeeaeaeaeieateee eeetee ee eeue eeee.;eeeeeieeeeeaeeeee e eeee eeeeeeeo,e \n",
            "\n",
            "[36.80793833732605 (200 10%) 2.0299]\n",
            "Whteieetae eeieeeet eeeettee aeaeeete i eiteie  e  ieetea e ae ie t' e  eai  e ee  et ee ioeeeeaieaiee \n",
            "\n",
            "[55.307148933410645 (300 15%) 2.1307]\n",
            "Whe  iei   ii  t i  ye ia .tee  ii  itieaei  taa ia eae ti i ttaea i a  ia ee  oiiiiit  o  iiea a aaia \n",
            "\n",
            "[73.83614993095398 (400 20%) 1.8632]\n",
            "Wheee   aa ie e    iei e eeee e eee.i i e oeety     i e i    ei os  i  , tei     t i t  t  ei ii a eee \n",
            "\n",
            "[92.35108065605164 (500 25%) 2.1072]\n",
            "Whe eeiaoeaaaiooii   aieoataeoiiea i aeeaoii iei oe meeieeoaeeaeie   e ee  eoeaeii a i teia .oeait iee \n",
            "\n",
            "[110.89212036132812 (600 30%) 1.8382]\n",
            "Who iieeieie ye etat  t eea  eeeieeeiei heto ea  tiii t iae e  s eaiaeie iieeeeieee  ietea ae t  it  t \n",
            "\n",
            "[129.3649935722351 (700 35%) 1.9604]\n",
            "Whee t t    e  e ee    ie    at  et ei e  e    t e s e. aa     i  eie i   i , ae  ei  eee te e. et i   \n",
            "\n",
            "[147.72273993492126 (800 40%) 1.9421]\n",
            "Wh i      i                     e                  te                 .                  t        a    \n",
            "\n",
            "[166.61132216453552 (900 45%) 1.9060]\n",
            "Wh ee   aea    e    et eii t.e ae   'ee   i i    t          ae  e a  at e e e  ei  ee i  au ee eee   t \n",
            "\n",
            "[185.2352294921875 (1000 50%) 1.7188]\n",
            "Who  iit a at    i             ae     t t     o e  ee       i        a   ,          a        i   a     \n",
            "\n",
            "[203.64812445640564 (1100 55%) 1.6608]\n",
            "Whiiae iie  e   e e    ea e eea   e ae  i eei e   . eie i a    i     e e  e        ese  te. a ie       \n",
            "\n",
            "[222.0716917514801 (1200 60%) 1.6990]\n",
            "Wh   - a  e ewee  i  ieasalta h ea,si   eeo t  o  et ea eea  oe  aieieeie.es te  tiae     a ii   i ete \n",
            "\n",
            "[240.64913988113403 (1300 65%) 1.6399]\n",
            "Wh iii ei  ieo ei ei iieeee i seae ?l et.ea.leei   iaeeyioh eeiiaie ii eii oaisi al e i ei e e ei eeee \n",
            "\n",
            "[259.04829597473145 (1400 70%) 1.6613]\n",
            "Wh laiooaeeo'i  iosee eeaoiia iaceeaeeiiiaeiie iie.laeii,eiteiiie oooaiteeeeotetii ia eieiaa loiet.eet \n",
            "\n",
            "[277.61146545410156 (1500 75%) 1.8880]\n",
            "Wh  iieiia slieiei ieaiei  aaeiaeeeaa ieaeee ieseeeaaehiayeeiae ia eaaiii iaea iieiyil ae iaa eeaaol   \n",
            "\n",
            "[296.0967147350311 (1600 80%) 1.6508]\n",
            "Wh ee a   o  ioeee iaeeeeeie o i  eae  l   e o e e,i  i e e  ei e  o eioe  a eme- aeeeeoes ieeeie eeai \n",
            "\n",
            "[314.6777129173279 (1700 85%) 1.6859]\n",
            "Wheee aaaoieeieaeeioe iiaiie oae eoteiioeeiea   iie  e eeaeaii  i eiie eeeiala oseai iai  a.  iea i  i \n",
            "\n",
            "[333.35322976112366 (1800 90%) 1.8901]\n",
            "Whetaiaiat e oaeeieaies!i  o  e ;eaeeaesioaeeoae aas,  iei air  eeeia .aeoaeseaeoaeeaoaieio oieaaaoeoa \n",
            "\n",
            "[351.9576885700226 (1900 95%) 1.5791]\n",
            "Wh    iiiie a,e  t a.   o eo e i s.eee ei   e     ee a . at   soe      e e  isa e e,ee iaa bi.a s  ee  \n",
            "\n",
            "[370.4894812107086 (2000 100%) 1.7264]\n",
            "Wheeeoie toee   ae, i    e oo  ee     s. i      e e  .  e  e e a,  s    e.oe e e  i eeeee, ee      e   \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}